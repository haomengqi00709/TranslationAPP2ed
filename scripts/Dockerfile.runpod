# RunPod Serverless Dockerfile
# Optimized for fast cold starts and GPU efficiency

FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    BERT_DEVICE=cuda \
    TRANSLATOR_TYPE=local

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    build-essential \
    cmake \
    git \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Note: fast_align should be copied from parent directory
# Since Docker can't access parent dirs, we'll install it differently
# Option 1: Clone from git
RUN git clone https://github.com/clab/fast_align.git && \
    cd fast_align && \
    mkdir -p build && \
    cd build && \
    cmake .. && \
    make -j$(nproc) && \
    cp fast_align atools /usr/local/bin/ && \
    cd /app && \
    rm -rf fast_align

# Option 2: If you prefer to copy local fast_align,
# you need to run docker build from parent directory instead:
# cd /Users/jasonhao/Desktop/fast_align_test
# docker build -f translationAPP_2ed/Dockerfile.runpod -t yourimage:tag .

# Install Python dependencies (use full requirements for RunPod)
COPY requirements-full.txt .
RUN pip install --no-cache-dir -r requirements-full.txt

# Install RunPod SDK
RUN pip install --no-cache-dir runpod

# Pre-download BERT model (saves time on cold starts)
RUN python3 -c "from transformers import AutoTokenizer, AutoModel; \
    model_name='sentence-transformers/LaBSE'; \
    AutoTokenizer.from_pretrained(model_name); \
    AutoModel.from_pretrained(model_name)"

# Pre-download local LLM models (8B and 14B)
# NOTE: This is optional. Models will download at runtime if not pre-bundled.
# Uncomment to pre-download (requires 32GB+ Docker memory allocation):
# RUN python3 -c "from transformers import AutoTokenizer, AutoModelForCausalLM; \
#     print('Downloading Qwen3-8B...'); \
#     AutoTokenizer.from_pretrained('Qwen/Qwen3-8B'); \
#     AutoModelForCausalLM.from_pretrained('Qwen/Qwen3-8B'); \
#     print('Downloading Qwen3-14B...'); \
#     AutoTokenizer.from_pretrained('Qwen/Qwen3-14B'); \
#     AutoModelForCausalLM.from_pretrained('Qwen/Qwen3-14B'); \
#     print('All models downloaded!')"

# Copy application code
COPY . .

# Create necessary directories
RUN mkdir -p temp logs

# Set the handler
CMD ["python3", "runpod_handler.py"]
